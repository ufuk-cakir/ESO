{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_conv_output_dim(layer: nn.Module, input_dim: tuple) -> tuple:\n",
    "    \"\"\"Calculate output dimension of a CNN layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : torch.nn.Module\n",
    "        The CNN layer to calculate the output dimension of\n",
    "    input_dim : tuple\n",
    "        The input dimension of the CNN layer in the form of (n_channels, height, width)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The output dimension of the CNN layer in the form of (n_channels, height, width)\n",
    "    \"\"\"\n",
    "    kernel_size = layer.kernel_size\n",
    "    stride = layer.stride\n",
    "    padding = layer.padding\n",
    "    dilation = layer.dilation\n",
    "\n",
    "    input_channels, input_height, input_width = input_dim\n",
    "\n",
    "    output_channels = layer.out_channels\n",
    "    output_height = (\n",
    "        input_height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1\n",
    "    ) / stride[0] + 1\n",
    "    output_width = (\n",
    "        input_width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1\n",
    "    ) / stride[1] + 1\n",
    "\n",
    "    return (output_channels, int(output_height), int(output_width))\n",
    "\n",
    "\n",
    "class BaseCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        conv_layers,\n",
    "        conv_filters,\n",
    "        dropout_rate,\n",
    "        conv_kernel,\n",
    "        max_pooling_size,\n",
    "        fc_units,\n",
    "        fc_layers,\n",
    "    ):\n",
    "        \"\"\"Base CNN model for the classification of the images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            The input shape of the images in the form of (n_channels, height, width)\n",
    "        conv_layers : int\n",
    "            The number of convolutional layers\n",
    "        conv_filters : int\n",
    "            The number of filters in the convolutional layers\n",
    "        dropout_rate : float\n",
    "            The dropout rate of the dropout layers\n",
    "        conv_kernel : int\n",
    "            The kernel size of the convolutional layers\n",
    "        max_pooling_size : int\n",
    "            The kernel size of the max pooling layers\n",
    "        fc_units : int\n",
    "            The number of units in the fully connected layers\n",
    "        fc_layers : int\n",
    "            The number of fully connected layers\n",
    "        \"\"\"\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        n_channels = input_shape[0]\n",
    "        self.n_conv_layers = conv_layers\n",
    "        self.conv_filters = conv_filters\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.max_pooling_size = max_pooling_size\n",
    "        self.fc_units = fc_units\n",
    "        self.n_fc_layers = fc_layers\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential()\n",
    "        self.conv_layers.add_module(\n",
    "            \"conv0\",\n",
    "            nn.Conv2d(n_channels, self.conv_filters, kernel_size=self.conv_kernel),\n",
    "        )\n",
    "        self.conv_layers.add_module(\"relu0\", nn.ReLU())\n",
    "        self.conv_layers.add_module(\"dropout0\", nn.Dropout(self.dropout_rate))\n",
    "        self.conv_layers.add_module(\"maxpool0\", nn.MaxPool2d(self.max_pooling_size))\n",
    "\n",
    "        for i in range(1, self.n_conv_layers):\n",
    "            self.conv_layers.add_module(\n",
    "                f\"conv{i}\",\n",
    "                nn.Conv2d(\n",
    "                    self.conv_filters, self.conv_filters, kernel_size=self.conv_kernel\n",
    "                ),\n",
    "            )\n",
    "            self.conv_layers.add_module(f\"relu{i}\", nn.ReLU())\n",
    "            self.conv_layers.add_module(f\"dropout{i}\", nn.Dropout(self.dropout_rate))\n",
    "            self.conv_layers.add_module(\n",
    "                f\"maxpool{i}\", nn.MaxPool2d(self.max_pooling_size)\n",
    "            )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential()\n",
    "        # input_units = self.conv_filters * (128 // (self.max_pooling_size ** self.n_conv_layers)) * (76 // (self.max_pooling_size ** self.n_conv_layers))\n",
    "        input_units = np.prod(self._calc_cnn_output_dim())\n",
    "        for i in range(self.n_fc_layers):\n",
    "            self.fc_layers.add_module(f\"fc{i}\", nn.Linear(input_units, self.fc_units))\n",
    "            self.fc_layers.add_module(f\"relu{i}\", nn.ReLU())\n",
    "            self.fc_layers.add_module(f\"dropout{i}\", nn.Dropout(self.dropout_rate))\n",
    "            input_units = self.fc_units\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.fc_units, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _calc_cnn_output_dim(self) -> tuple:\n",
    "        \"\"\"Calculate output dimension of the CNN part of the network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "        The output dimension of the CNN part of the network in the form of (n_channels, height, width)\n",
    "        \"\"\"\n",
    "        output_dim = get_conv_output_dim(self.conv_layers[0], self.input_shape)\n",
    "        for layer in self.conv_layers[1:]:\n",
    "            # Check if layer is a convolutional layer\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                output_dim = get_conv_output_dim(layer, output_dim)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                output_dim = (\n",
    "                    output_dim[0],\n",
    "                    output_dim[1] // layer.kernel_size,\n",
    "                    output_dim[2] // layer.kernel_size,\n",
    "                )\n",
    "\n",
    "        return output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor. Shape should be (batch_size, n_channels, height, width)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor. Shape should be (batch_size, n_classes). Outputs a probability for each class.\n",
    "        \"\"\"\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        # print(\"x size: \", x.size())\n",
    "        x = self.fc_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cnn import BaseCNN\n",
    "\n",
    "conv_layers = 1\n",
    "fc_layers = 2\n",
    "max_pooling_size = 4\n",
    "dropout_rate = 0.5\n",
    "conv_filters = 8\n",
    "conv_kernel = 8\n",
    "fc_units = 32\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Model class.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        batch_size=32,\n",
    "        optimizer=\"adam\",\n",
    "        learning_rate=0.001,\n",
    "        loss=\"cross_entropy\",\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        self.cnn = BaseCNN(\n",
    "            input_shape=input_shape,\n",
    "            conv_layers=conv_layers,\n",
    "            conv_filters=conv_filters,\n",
    "            dropout_rate=dropout_rate,\n",
    "            conv_kernel=conv_kernel,\n",
    "            max_pooling_size=max_pooling_size,\n",
    "            fc_units=fc_units,\n",
    "            fc_layers=fc_layers,\n",
    "        )\n",
    "\n",
    "        # self.logger.info(\"Initializing Model...\")\n",
    "        # Get Device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.optimizer_name = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_name = loss\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self._set_optimizer_and_loss()\n",
    "\n",
    "    def _set_optimizer_and_loss(self):\n",
    "        \"\"\"Set the optimizer and loss function\"\"\"\n",
    "        if self.optimizer_name == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                self.cnn.parameters(), lr=self.learning_rate\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only Adam optimizer is supported at the moment\")\n",
    "\n",
    "        if self.loss_name == \"cross_entropy\":\n",
    "            self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Only cross entropy loss is supported at the moment\"\n",
    "            )\n",
    "\n",
    "    def get_number_of_parameters(self):\n",
    "        return sum(p.numel() for p in self.cnn.parameters() if p.requires_grad)\n",
    "\n",
    "    def _create_dataloader(\n",
    "        self, X: np.array, Y: np.array\n",
    "    ) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Create a dataloader from the given data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data of shape (n_samples,height,width) or (n_samples,channels,height,width). Will add channel dimension if needed.\n",
    "        Y : np.array\n",
    "            Target data of shape (n_samples,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loader : torch.utils.data.DataLoader\n",
    "            Dataloader with the given data and batch size specified in the constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        X_tensor = torch.from_numpy(X).float()\n",
    "        Y_tensor = torch.from_numpy(Y).float()\n",
    "\n",
    "        # Reshape X_tensor\n",
    "        if len(X_tensor.shape) == 3:\n",
    "            X_tensor = X_tensor.unsqueeze(1)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=self.shuffle\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def train(self, n_epochs, X_train, Y_train, save_path=None, verbose=False):\n",
    "        print(\"Training\")\n",
    "\n",
    "        # Create Dataloaders\n",
    "        train_loader = self._create_dataloader(X_train, Y_train)\n",
    "        self.cnn.to(self.device)\n",
    "        train_losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch: \", epoch)\n",
    "            for batch_inputs, batch_targets in train_loader:\n",
    "                batch_inputs, batch_targets = (\n",
    "                    batch_inputs.to(self.device),\n",
    "                    batch_targets.to(self.device),\n",
    "                )\n",
    "                # Reset gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                batch_preds = self.cnn.forward(batch_inputs)\n",
    "                # Compute loss\n",
    "                loss = self.criterion(batch_preds, batch_targets)\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "                # Validation\n",
    "                if epoch % 10 == 0:\n",
    "                    pass\n",
    "                    \"\"\"\n",
    "                    val_loss, val_acc = self.evaluate(val_loader, criterion)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accs.append(val_acc)\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                    \"\"\"\n",
    "                    if verbose:\n",
    "                        print(f\"Epoch {epoch} | Train Loss {loss.item()}\")\n",
    "        return train_losses  # , val_losses, val_accs\n",
    "\n",
    "    def evaluate(self, X_test, Y_test, criterion):\n",
    "        loader = self._create_dataloader(X=X_test, Y=Y_test)\n",
    "        self.cnn.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for batch_inputs, batch_targets in loader:\n",
    "                batch_inputs, batch_targets = (\n",
    "                    batch_inputs.to(self.device),\n",
    "                    batch_targets.to(self.device),\n",
    "                )\n",
    "                batch_preds = self.cnn.forward(batch_inputs)\n",
    "\n",
    "                total_loss += self.criterion(batch_preds, batch_targets).item()\n",
    "                class_predictions = batch_preds.argmax(dim=1)\n",
    "                # true classes are one hot encoded\n",
    "                true_classes = batch_targets.argmax(dim=1)\n",
    "\n",
    "                correct += (class_predictions == true_classes).sum().item()\n",
    "            average_loss = total_loss / len(loader)\n",
    "            accuracy = correct / len(loader.dataset)\n",
    "        self.cnn.train()\n",
    "        return average_loss, accuracy\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.cnn)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "datapath = (\n",
    "    \"/Users/ufuk/1. Research/AIMS/Project Repo/eso/data/SavedData/preprocessed/train\"\n",
    ")\n",
    "\n",
    "with open(datapath + \"/X.pkl\", \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "with open(datapath + \"/Y.pkl\", \"rb\") as f:\n",
    "    Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 128, 76)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 76, 1])\n"
     ]
    }
   ],
   "source": [
    "test = torch.from_numpy(X[0]).float()\n",
    "test = test.unsqueeze(2)\n",
    "test = test.unsqueeze(0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66792"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 * 121 * 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 76)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, X.shape[1], X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4080"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 * 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4080"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 * 30 * 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 76])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4863, 0.5137]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model(input_shape=(1, X.shape[1], X.shape[2]))\n",
    "test = torch.from_numpy(X[0]).float()\n",
    "test = test.unsqueeze(0)\n",
    "test = test.unsqueeze(0)\n",
    "print(test.shape)\n",
    "m(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(input_shape=(1, X.shape[1], X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 76])\n"
     ]
    }
   ],
   "source": [
    "m = Model(input_shape=(1, X.shape[1], X.shape[2]))\n",
    "test = torch.from_numpy(X[0]).float()\n",
    "test = test.unsqueeze(0)\n",
    "test = test.unsqueeze(0)\n",
    "print(test.shape)\n",
    "m(test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "y_test_tensor = torch.Tensor(y_test)\n",
    "\n",
    "\n",
    "# Reshape input data if needed\n",
    "if len(X_train_tensor.shape) == 3:\n",
    "    X_train_tensor = X_train_tensor.unsqueeze(1)\n",
    "    X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1584, 1, 128, 76])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch:  0\n",
      "Epoch 0 | Train Loss 0.41014260053634644\n",
      "Epoch 0 | Train Loss 0.581453263759613\n",
      "Epoch 0 | Train Loss 0.5128495693206787\n",
      "Epoch 0 | Train Loss 0.4777698218822479\n",
      "Epoch 0 | Train Loss 0.5446839928627014\n",
      "Epoch 0 | Train Loss 0.47392070293426514\n",
      "Epoch 0 | Train Loss 0.5632570385932922\n",
      "Epoch 0 | Train Loss 0.567483127117157\n",
      "Epoch 0 | Train Loss 0.5461961030960083\n",
      "Epoch 0 | Train Loss 0.5639830231666565\n",
      "Epoch 0 | Train Loss 0.4779053330421448\n",
      "Epoch 0 | Train Loss 0.6133934855461121\n",
      "Epoch 0 | Train Loss 0.6607391834259033\n",
      "Epoch 0 | Train Loss 0.4760320782661438\n",
      "Epoch 0 | Train Loss 0.4772844612598419\n",
      "Epoch 0 | Train Loss 0.5231408476829529\n",
      "Epoch 0 | Train Loss 0.591839611530304\n",
      "Epoch 0 | Train Loss 0.6541820168495178\n",
      "Epoch 0 | Train Loss 0.5608392357826233\n",
      "Epoch 0 | Train Loss 0.4975656270980835\n",
      "Epoch 0 | Train Loss 0.6153240203857422\n",
      "Epoch 0 | Train Loss 0.6527336239814758\n",
      "Epoch 0 | Train Loss 0.4122413396835327\n",
      "Epoch 0 | Train Loss 0.5761924982070923\n",
      "Epoch 0 | Train Loss 0.545682430267334\n",
      "Epoch 0 | Train Loss 0.5528038144111633\n",
      "Epoch 0 | Train Loss 0.5689399838447571\n",
      "Epoch 0 | Train Loss 0.6199493408203125\n",
      "Epoch 0 | Train Loss 0.4700740575790405\n",
      "Epoch 0 | Train Loss 0.4096098840236664\n",
      "Epoch 0 | Train Loss 0.49720707535743713\n",
      "Epoch 0 | Train Loss 0.5574128031730652\n",
      "Epoch 0 | Train Loss 0.5570170283317566\n",
      "Epoch 0 | Train Loss 0.4795774817466736\n",
      "Epoch 0 | Train Loss 0.45046019554138184\n",
      "Epoch 0 | Train Loss 0.7358554601669312\n",
      "Epoch 0 | Train Loss 0.45919686555862427\n",
      "Epoch 0 | Train Loss 0.4685066044330597\n",
      "Epoch 0 | Train Loss 0.5697107911109924\n",
      "Epoch 0 | Train Loss 0.5285966992378235\n",
      "Epoch 0 | Train Loss 0.5515962243080139\n",
      "Epoch 0 | Train Loss 0.5350027084350586\n",
      "Epoch 0 | Train Loss 0.4814305901527405\n",
      "Epoch 0 | Train Loss 0.5748869776725769\n",
      "Epoch 0 | Train Loss 0.4422222375869751\n",
      "Epoch 0 | Train Loss 0.4810870885848999\n",
      "Epoch 0 | Train Loss 0.543252170085907\n",
      "Epoch 0 | Train Loss 0.47231951355934143\n",
      "Epoch 0 | Train Loss 0.5182859897613525\n",
      "Epoch 0 | Train Loss 0.49845290184020996\n",
      "Epoch:  1\n",
      "Epoch:  2\n",
      "Epoch:  3\n",
      "Epoch:  4\n",
      "Epoch:  5\n",
      "Epoch:  6\n",
      "Epoch:  7\n",
      "Epoch:  8\n",
      "Epoch:  9\n",
      "Epoch:  10\n",
      "Epoch 10 | Train Loss 0.3944215476512909\n",
      "Epoch 10 | Train Loss 0.40094083547592163\n",
      "Epoch 10 | Train Loss 0.4975125789642334\n",
      "Epoch 10 | Train Loss 0.40375852584838867\n",
      "Epoch 10 | Train Loss 0.3768002986907959\n",
      "Epoch 10 | Train Loss 0.4472280740737915\n",
      "Epoch 10 | Train Loss 0.34715840220451355\n",
      "Epoch 10 | Train Loss 0.40628495812416077\n",
      "Epoch 10 | Train Loss 0.437461793422699\n",
      "Epoch 10 | Train Loss 0.3342202305793762\n",
      "Epoch 10 | Train Loss 0.35593438148498535\n",
      "Epoch 10 | Train Loss 0.37907347083091736\n",
      "Epoch 10 | Train Loss 0.3681958019733429\n",
      "Epoch 10 | Train Loss 0.5038327574729919\n",
      "Epoch 10 | Train Loss 0.3570882976055145\n",
      "Epoch 10 | Train Loss 0.32066139578819275\n",
      "Epoch 10 | Train Loss 0.3525182604789734\n",
      "Epoch 10 | Train Loss 0.35750314593315125\n",
      "Epoch 10 | Train Loss 0.3603992164134979\n",
      "Epoch 10 | Train Loss 0.35779327154159546\n",
      "Epoch 10 | Train Loss 0.42915844917297363\n",
      "Epoch 10 | Train Loss 0.4124806821346283\n",
      "Epoch 10 | Train Loss 0.4148426949977875\n",
      "Epoch 10 | Train Loss 0.4340445399284363\n",
      "Epoch 10 | Train Loss 0.443573534488678\n",
      "Epoch 10 | Train Loss 0.5251827239990234\n",
      "Epoch 10 | Train Loss 0.361747682094574\n",
      "Epoch 10 | Train Loss 0.33741188049316406\n",
      "Epoch 10 | Train Loss 0.34608134627342224\n",
      "Epoch 10 | Train Loss 0.40976786613464355\n",
      "Epoch 10 | Train Loss 0.37116095423698425\n",
      "Epoch 10 | Train Loss 0.3564215898513794\n",
      "Epoch 10 | Train Loss 0.3528294563293457\n",
      "Epoch 10 | Train Loss 0.3773220181465149\n",
      "Epoch 10 | Train Loss 0.41743239760398865\n",
      "Epoch 10 | Train Loss 0.41958388686180115\n",
      "Epoch 10 | Train Loss 0.37002032995224\n",
      "Epoch 10 | Train Loss 0.3458525240421295\n",
      "Epoch 10 | Train Loss 0.3289959132671356\n",
      "Epoch 10 | Train Loss 0.38590237498283386\n",
      "Epoch 10 | Train Loss 0.36694395542144775\n",
      "Epoch 10 | Train Loss 0.3324372172355652\n",
      "Epoch 10 | Train Loss 0.44651269912719727\n",
      "Epoch 10 | Train Loss 0.34484782814979553\n",
      "Epoch 10 | Train Loss 0.3408869802951813\n",
      "Epoch 10 | Train Loss 0.37402400374412537\n",
      "Epoch 10 | Train Loss 0.4070766866207123\n",
      "Epoch 10 | Train Loss 0.35516637563705444\n",
      "Epoch 10 | Train Loss 0.3461560308933258\n",
      "Epoch 10 | Train Loss 0.3751280903816223\n",
      "Epoch:  11\n",
      "Epoch:  12\n",
      "Epoch:  13\n",
      "Epoch:  14\n",
      "Epoch:  15\n",
      "Epoch:  16\n",
      "Epoch:  17\n",
      "Epoch:  18\n",
      "Epoch:  19\n",
      "Epoch:  20\n",
      "Epoch 20 | Train Loss 0.315263569355011\n",
      "Epoch 20 | Train Loss 0.3173661231994629\n",
      "Epoch 20 | Train Loss 0.3430544435977936\n",
      "Epoch 20 | Train Loss 0.3133814334869385\n",
      "Epoch 20 | Train Loss 0.3404076397418976\n",
      "Epoch 20 | Train Loss 0.33438926935195923\n",
      "Epoch 20 | Train Loss 0.3220575749874115\n",
      "Epoch 20 | Train Loss 0.3151242136955261\n",
      "Epoch 20 | Train Loss 0.31607338786125183\n",
      "Epoch 20 | Train Loss 0.3136909306049347\n",
      "Epoch 20 | Train Loss 0.3152834475040436\n",
      "Epoch 20 | Train Loss 0.3165183663368225\n",
      "Epoch 20 | Train Loss 0.3196665048599243\n",
      "Epoch 20 | Train Loss 0.31473100185394287\n",
      "Epoch 20 | Train Loss 0.32503417134284973\n",
      "Epoch 20 | Train Loss 0.31800025701522827\n",
      "Epoch 20 | Train Loss 0.3182762861251831\n",
      "Epoch 20 | Train Loss 0.31458890438079834\n",
      "Epoch 20 | Train Loss 0.31568628549575806\n",
      "Epoch 20 | Train Loss 0.3263319432735443\n",
      "Epoch 20 | Train Loss 0.3169516623020172\n",
      "Epoch 20 | Train Loss 0.31807464361190796\n",
      "Epoch 20 | Train Loss 0.31348714232444763\n",
      "Epoch 20 | Train Loss 0.3357800841331482\n",
      "Epoch 20 | Train Loss 0.322538822889328\n",
      "Epoch 20 | Train Loss 0.3180373013019562\n",
      "Epoch 20 | Train Loss 0.31363728642463684\n",
      "Epoch 20 | Train Loss 0.31569811701774597\n",
      "Epoch 20 | Train Loss 0.31371569633483887\n",
      "Epoch 20 | Train Loss 0.3195371627807617\n",
      "Epoch 20 | Train Loss 0.3156048059463501\n",
      "Epoch 20 | Train Loss 0.3133240044116974\n",
      "Epoch 20 | Train Loss 0.32381242513656616\n",
      "Epoch 20 | Train Loss 0.3184601068496704\n",
      "Epoch 20 | Train Loss 0.33074966073036194\n",
      "Epoch 20 | Train Loss 0.3285927176475525\n",
      "Epoch 20 | Train Loss 0.3201987147331238\n",
      "Epoch 20 | Train Loss 0.31581979990005493\n",
      "Epoch 20 | Train Loss 0.3164348304271698\n",
      "Epoch 20 | Train Loss 0.31940147280693054\n",
      "Epoch 20 | Train Loss 0.3284722566604614\n",
      "Epoch 20 | Train Loss 0.3285718560218811\n",
      "Epoch 20 | Train Loss 0.31424492597579956\n",
      "Epoch 20 | Train Loss 0.3170992434024811\n",
      "Epoch 20 | Train Loss 0.34249648451805115\n",
      "Epoch 20 | Train Loss 0.3213164508342743\n",
      "Epoch 20 | Train Loss 0.3164096176624298\n",
      "Epoch 20 | Train Loss 0.32654568552970886\n",
      "Epoch 20 | Train Loss 0.34114429354667664\n",
      "Epoch 20 | Train Loss 0.31366318464279175\n",
      "Epoch:  21\n",
      "Epoch:  22\n",
      "Epoch:  23\n",
      "Epoch:  24\n",
      "Epoch:  25\n",
      "Epoch:  26\n",
      "Epoch:  27\n",
      "Epoch:  28\n",
      "Epoch:  29\n",
      "Epoch:  30\n",
      "Epoch 30 | Train Loss 0.3142120838165283\n",
      "Epoch 30 | Train Loss 0.33704936504364014\n",
      "Epoch 30 | Train Loss 0.31954658031463623\n",
      "Epoch 30 | Train Loss 0.31903690099716187\n",
      "Epoch 30 | Train Loss 0.33995604515075684\n",
      "Epoch 30 | Train Loss 0.3173440098762512\n",
      "Epoch 30 | Train Loss 0.32613202929496765\n",
      "Epoch 30 | Train Loss 0.3147391676902771\n",
      "Epoch 30 | Train Loss 0.3137168288230896\n",
      "Epoch 30 | Train Loss 0.31873685121536255\n",
      "Epoch 30 | Train Loss 0.313334196805954\n",
      "Epoch 30 | Train Loss 0.3132782280445099\n",
      "Epoch 30 | Train Loss 0.31580063700675964\n",
      "Epoch 30 | Train Loss 0.3211667835712433\n",
      "Epoch 30 | Train Loss 0.31363001465797424\n",
      "Epoch 30 | Train Loss 0.3438372015953064\n",
      "Epoch 30 | Train Loss 0.3134268522262573\n",
      "Epoch 30 | Train Loss 0.31785768270492554\n",
      "Epoch 30 | Train Loss 0.35900288820266724\n",
      "Epoch 30 | Train Loss 0.31571102142333984\n",
      "Epoch 30 | Train Loss 0.33914700150489807\n",
      "Epoch 30 | Train Loss 0.3147042691707611\n",
      "Epoch 30 | Train Loss 0.31340113282203674\n",
      "Epoch 30 | Train Loss 0.3132622241973877\n",
      "Epoch 30 | Train Loss 0.3140478730201721\n",
      "Epoch 30 | Train Loss 0.3388845920562744\n",
      "Epoch 30 | Train Loss 0.3134925663471222\n",
      "Epoch 30 | Train Loss 0.3133341670036316\n",
      "Epoch 30 | Train Loss 0.31404614448547363\n",
      "Epoch 30 | Train Loss 0.3133786618709564\n",
      "Epoch 30 | Train Loss 0.3136448264122009\n",
      "Epoch 30 | Train Loss 0.3146142065525055\n",
      "Epoch 30 | Train Loss 0.3138173222541809\n",
      "Epoch 30 | Train Loss 0.34646642208099365\n",
      "Epoch 30 | Train Loss 0.3144308924674988\n",
      "Epoch 30 | Train Loss 0.3182300925254822\n",
      "Epoch 30 | Train Loss 0.3207481801509857\n",
      "Epoch 30 | Train Loss 0.3207797408103943\n",
      "Epoch 30 | Train Loss 0.3144841194152832\n",
      "Epoch 30 | Train Loss 0.3154490292072296\n",
      "Epoch 30 | Train Loss 0.3139517903327942\n",
      "Epoch 30 | Train Loss 0.3141019940376282\n",
      "Epoch 30 | Train Loss 0.3146250247955322\n",
      "Epoch 30 | Train Loss 0.34269970655441284\n",
      "Epoch 30 | Train Loss 0.3135090172290802\n",
      "Epoch 30 | Train Loss 0.3156987130641937\n",
      "Epoch 30 | Train Loss 0.3160630166530609\n",
      "Epoch 30 | Train Loss 0.3423934280872345\n",
      "Epoch 30 | Train Loss 0.3136289715766907\n",
      "Epoch 30 | Train Loss 0.3138657808303833\n",
      "Epoch:  31\n",
      "Epoch:  32\n",
      "Epoch:  33\n",
      "Epoch:  34\n",
      "Epoch:  35\n",
      "Epoch:  36\n",
      "Epoch:  37\n",
      "Epoch:  38\n",
      "Epoch:  39\n",
      "Epoch:  40\n",
      "Epoch 40 | Train Loss 0.3190793991088867\n",
      "Epoch 40 | Train Loss 0.31460118293762207\n",
      "Epoch 40 | Train Loss 0.3236154317855835\n",
      "Epoch 40 | Train Loss 0.31326955556869507\n",
      "Epoch 40 | Train Loss 0.3139191269874573\n",
      "Epoch 40 | Train Loss 0.3145836591720581\n",
      "Epoch 40 | Train Loss 0.3132765293121338\n",
      "Epoch 40 | Train Loss 0.314134806394577\n",
      "Epoch 40 | Train Loss 0.3132622241973877\n",
      "Epoch 40 | Train Loss 0.3196335434913635\n",
      "Epoch 40 | Train Loss 0.314365953207016\n",
      "Epoch 40 | Train Loss 0.315667062997818\n",
      "Epoch 40 | Train Loss 0.3134545683860779\n",
      "Epoch 40 | Train Loss 0.3151152431964874\n",
      "Epoch 40 | Train Loss 0.3149312734603882\n",
      "Epoch 40 | Train Loss 0.36832067370414734\n",
      "Epoch 40 | Train Loss 0.3148891031742096\n",
      "Epoch 40 | Train Loss 0.3193580210208893\n",
      "Epoch 40 | Train Loss 0.3132983446121216\n",
      "Epoch 40 | Train Loss 0.3132653832435608\n",
      "Epoch 40 | Train Loss 0.31331396102905273\n",
      "Epoch 40 | Train Loss 0.3164806067943573\n",
      "Epoch 40 | Train Loss 0.3136522173881531\n",
      "Epoch 40 | Train Loss 0.31326165795326233\n",
      "Epoch 40 | Train Loss 0.31432390213012695\n",
      "Epoch 40 | Train Loss 0.3143470883369446\n",
      "Epoch 40 | Train Loss 0.3137378394603729\n",
      "Epoch 40 | Train Loss 0.3132665455341339\n",
      "Epoch 40 | Train Loss 0.3185230791568756\n",
      "Epoch 40 | Train Loss 0.3138437569141388\n",
      "Epoch 40 | Train Loss 0.31378960609436035\n",
      "Epoch 40 | Train Loss 0.31326180696487427\n",
      "Epoch 40 | Train Loss 0.31326165795326233\n",
      "Epoch 40 | Train Loss 0.31345146894454956\n",
      "Epoch 40 | Train Loss 0.31394481658935547\n",
      "Epoch 40 | Train Loss 0.3133156895637512\n",
      "Epoch 40 | Train Loss 0.31359246373176575\n",
      "Epoch 40 | Train Loss 0.314581036567688\n",
      "Epoch 40 | Train Loss 0.34321537613868713\n",
      "Epoch 40 | Train Loss 0.3132627606391907\n",
      "Epoch 40 | Train Loss 0.3132619261741638\n",
      "Epoch 40 | Train Loss 0.3132902979850769\n",
      "Epoch 40 | Train Loss 0.3133983910083771\n",
      "Epoch 40 | Train Loss 0.31424984335899353\n",
      "Epoch 40 | Train Loss 0.3132661283016205\n",
      "Epoch 40 | Train Loss 0.317167729139328\n",
      "Epoch 40 | Train Loss 0.3132616877555847\n",
      "Epoch 40 | Train Loss 0.3155668079853058\n",
      "Epoch 40 | Train Loss 0.3133196234703064\n",
      "Epoch 40 | Train Loss 0.31416845321655273\n",
      "Epoch:  41\n",
      "Epoch:  42\n",
      "Epoch:  43\n",
      "Epoch:  44\n",
      "Epoch:  45\n",
      "Epoch:  46\n",
      "Epoch:  47\n",
      "Epoch:  48\n",
      "Epoch:  49\n",
      "Epoch:  50\n",
      "Epoch 50 | Train Loss 0.3132617473602295\n",
      "Epoch 50 | Train Loss 0.3420092761516571\n",
      "Epoch 50 | Train Loss 0.313262939453125\n",
      "Epoch 50 | Train Loss 0.31329137086868286\n",
      "Epoch 50 | Train Loss 0.3132616877555847\n",
      "Epoch 50 | Train Loss 0.31365567445755005\n",
      "Epoch 50 | Train Loss 0.31326356530189514\n",
      "Epoch 50 | Train Loss 0.31364086270332336\n",
      "Epoch 50 | Train Loss 0.3137301802635193\n",
      "Epoch 50 | Train Loss 0.31326165795326233\n",
      "Epoch 50 | Train Loss 0.3142249882221222\n",
      "Epoch 50 | Train Loss 0.3138110637664795\n",
      "Epoch 50 | Train Loss 0.34017959237098694\n",
      "Epoch 50 | Train Loss 0.3132832944393158\n",
      "Epoch 50 | Train Loss 0.3133511543273926\n",
      "Epoch 50 | Train Loss 0.3152032196521759\n",
      "Epoch 50 | Train Loss 0.31326547265052795\n",
      "Epoch 50 | Train Loss 0.3159829378128052\n",
      "Epoch 50 | Train Loss 0.3136427402496338\n",
      "Epoch 50 | Train Loss 0.31326690316200256\n",
      "Epoch 50 | Train Loss 0.3133053779602051\n",
      "Epoch 50 | Train Loss 0.31326383352279663\n",
      "Epoch 50 | Train Loss 0.3132951855659485\n",
      "Epoch 50 | Train Loss 0.3132626414299011\n",
      "Epoch 50 | Train Loss 0.31902238726615906\n",
      "Epoch 50 | Train Loss 0.31367596983909607\n",
      "Epoch 50 | Train Loss 0.3132631182670593\n",
      "Epoch 50 | Train Loss 0.3145340383052826\n",
      "Epoch 50 | Train Loss 0.3132617771625519\n",
      "Epoch 50 | Train Loss 0.3133905827999115\n",
      "Epoch 50 | Train Loss 0.31326165795326233\n",
      "Epoch 50 | Train Loss 0.3132616877555847\n",
      "Epoch 50 | Train Loss 0.3133038878440857\n",
      "Epoch 50 | Train Loss 0.31326204538345337\n",
      "Epoch 50 | Train Loss 0.31326165795326233\n",
      "Epoch 50 | Train Loss 0.3133831322193146\n",
      "Epoch 50 | Train Loss 0.31329619884490967\n",
      "Epoch 50 | Train Loss 0.31326422095298767\n",
      "Epoch 50 | Train Loss 0.3142692744731903\n",
      "Epoch 50 | Train Loss 0.3142359256744385\n",
      "Epoch 50 | Train Loss 0.319907546043396\n",
      "Epoch 50 | Train Loss 0.3421887755393982\n",
      "Epoch 50 | Train Loss 0.3133191466331482\n",
      "Epoch 50 | Train Loss 0.3132655620574951\n",
      "Epoch 50 | Train Loss 0.31832370162010193\n",
      "Epoch 50 | Train Loss 0.3134937286376953\n",
      "Epoch 50 | Train Loss 0.3136453628540039\n",
      "Epoch 50 | Train Loss 0.3434978127479553\n",
      "Epoch 50 | Train Loss 0.3132641613483429\n",
      "Epoch 50 | Train Loss 0.3134933114051819\n",
      "Epoch:  51\n",
      "Epoch:  52\n",
      "Epoch:  53\n",
      "Epoch:  54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ufuk/1. Research/AIMS/Project Repo/eso/src/eso/model/new_model.ipynb Zelle 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m\u001b[39m.\u001b[39;49mtrain(\u001b[39m100\u001b[39;49m, X_train\u001b[39m=\u001b[39;49m X_train, Y_train \u001b[39m=\u001b[39;49m y_train, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/ufuk/1. Research/AIMS/Project Repo/eso/src/eso/model/new_model.ipynb Zelle 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m batch_preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn\u001b[39m.\u001b[39;49mforward(batch_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(batch_preds, batch_targets)\n",
      "\u001b[1;32m/Users/ufuk/1. Research/AIMS/Project Repo/eso/src/eso/model/new_model.ipynb Zelle 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_layers(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# Flatten the tensor\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/ufuk/1.%20Research/AIMS/Project%20Repo/eso/src/eso/model/new_model.ipynb#X10sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     \u001b[39m#print(\"x size: \", x.size())\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eso/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/eso/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/eso/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/eso/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/eso/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m.train(100, X_train=X_train, Y_train=y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3133186652110173, 1.0)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(X_test, y_test, m.criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_label:  [0. 1.]\n",
      "tensor([[1.5201e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.2157e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.2161e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.8747e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.3711e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 8.6611e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1480e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.5265e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.9732e-10]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 4.1014e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.7585e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9997e-01, 3.3947e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.8420e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.7562e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.5547e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3080e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.5821e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4121e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9471e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.1108e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1334e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.8059e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.2104e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1989e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0684e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.5775e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.5466e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.2249e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0353e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.7564e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.5162e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.1732e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3339e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.0416e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.8491e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5035e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.4722e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.3149e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1761e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.8096e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9193e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.6103e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.5748e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.0363e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.1337e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.8747e-09]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.3241e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.0611e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.5529e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.8108e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0080e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4257e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.7549e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.0935e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 7.0369e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 3.0201e-09]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.4851e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.6334e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0278e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9977e-01, 2.2764e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.9965e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7201e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[0.9984, 0.0016]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 3.0465e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5825e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.5491e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.8179e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.6793e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.7058e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.0941e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.1535e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0039e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.3577e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.2434e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.3054e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9758e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9178e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0465e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.4338e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0061e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 9.8766e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.3132e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.5634e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9996e-01, 4.4738e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9998e-01, 1.7343e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.1637e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 3.3728e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 9.3621e-10]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9926e-01, 7.4033e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9995e-01, 4.6933e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.6405e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.8110e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.5676e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.8635e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.7306e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.1254e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7910e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.6889e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6603e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.0101e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.1908e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 1.3383e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9998e-01, 2.4546e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.8785e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.4710e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.7507e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 6.0157e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.2117e-08, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.1357e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.1644e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.1493e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.8259e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.2988e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4385e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 4.7122e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4841e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.2904e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.1148e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.6640e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.9353e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.3178e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6931e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.3804e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.1988e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7085e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.8640e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.2518e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 8.1414e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[0.9847, 0.0153]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.3052e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3612e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.7401e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6378e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.4644e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9246e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4374e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.3327e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.4317e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.9787e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.3185e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9988e-01, 1.2316e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0671e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.0075e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.3314e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.7413e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0315e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6913e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5477e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.1619e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 4.0211e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.8855e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.4406e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3313e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9980e-01, 1.9707e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.7533e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.3261e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.3005e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 5.0278e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 5.0592e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.8652e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.1638e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.1527e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.0280e-08, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.8327e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.3714e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.7902e-09]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.3153e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0510e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 6.8464e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6164e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.2827e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 1.3540e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9987e-01, 1.2803e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.8017e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.8669e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.7938e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 3.1979e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9978e-01, 2.1779e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 5.1147e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5548e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6654e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.8488e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.0100e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.9938e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.4548e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.5711e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.2188e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0348e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.2992e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.3793e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.6669e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.3834e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0051e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 5.1294e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 5.1437e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.0932e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1007e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9926e-01, 7.4330e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.7316e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.2577e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.4879e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6771e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.2680e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.3076e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.9828e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4687e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.2024e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.2584e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9497e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1022e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9997e-01, 2.5833e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.0372e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.5746e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.6863e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6320e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.2168e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.3184e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0526e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 9.8483e-09]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9959e-01, 4.1199e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.1155e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.3658e-08, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4960e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.8321e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0678e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.6172e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.1753e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.3719e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.3084e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5022e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.2867e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.2275e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.0213e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 5.5727e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0282e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0407e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.5152e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.9363e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.6394e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 7.3025e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.8836e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.8512e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.5497e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5583e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 7.7178e-09]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.7291e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.3126e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6211e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 9.0267e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.8039e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.4470e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1439e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4078e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9950e-01, 4.9519e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.1308e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0000e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 4.2176e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 8.3619e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4378e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.3436e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.5700e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.3143e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.8387e-08, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 4.8774e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.7756e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6645e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.3821e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9509e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 6.9204e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.6329e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.2425e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.2339e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3004e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.9128e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.4690e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.5274e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7565e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6429e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.8165e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0931e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.8350e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.4192e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.0970e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.7340e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0094e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.0062e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9990e-01, 9.5969e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5042e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.9871e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7305e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.0858e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.3872e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.2665e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.7741e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0132e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.6274e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.2063e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.6314e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.0904e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.2661e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6700e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.4653e-08, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 3.6225e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.0196e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.2392e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7256e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.4053e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3696e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.5550e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9999e-01, 5.6995e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.1010e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.0335e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.0065e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4131e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5105e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1533e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.8388e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.1822e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.4794e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.3242e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.0021e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.9701e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7839e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3280e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.6652e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.5318e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.0036e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.0315e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.9702e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6056e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.3407e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.5386e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 8.1087e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5088e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 3.0470e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.1380e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.5086e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9998e-01, 2.0358e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.4143e-06]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0375e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.8007e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.4245e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9968e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.9486e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.9118e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6180e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.1580e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.0716e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.5682e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9960e-01, 3.9633e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[0.9781, 0.0219]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.4398e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.8444e-09]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.2214e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.0958e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9988e-01, 1.2335e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0723e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9968e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5022e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.0132e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.4797e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.8448e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 2.0388e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.8403e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 1.2773e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.9345e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.5605e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.0529e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0807e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.6272e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0358e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[9.9157e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.9771e-12, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.8509e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.9506e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.4068e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.9968e-18, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.5923e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[9.9936e-01, 6.3795e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.0342e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[7.8874e-19, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.3936e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 3.6410e-08]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.5444e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[8.2176e-08, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [1. 0.]\n",
      "tensor([[1.0000e+00, 6.3779e-09]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.2866e-14, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[2.6361e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.2709e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3931e-16, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.3012e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[1.7205e-15, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[5.0684e-17, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[4.4675e-09, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[6.7204e-13, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "true_label:  [0. 1.]\n",
      "tensor([[3.7979e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m.cnn.eval()\n",
    "for x, y in zip(X_test, y_test):\n",
    "    X = torch.from_numpy(x).float()\n",
    "    X = X.unsqueeze(0)\n",
    "    X = X.unsqueeze(0)\n",
    "    print(\"true_label: \", y)\n",
    "    predicted = m(X)\n",
    "    print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
